---
name: win-loss-analysis
description: Guides product marketers through designing, conducting, and reporting on win/loss analysis programs. Use when you need to understand why you win and lose deals, improve sales effectiveness, inform product roadmap, or build a systematic competitive intelligence program. Based on Clozd and Pragmatic Institute methodologies for interview design, synthesis, and stakeholder communication.
---

# Win/Loss Analysis Skill

## Overview
This skill helps product marketers design and run win/loss analysis programs that actually change sales behavior and inform product strategy. Win/loss interviews are one of the highest-leverage activities in product marketing — they provide direct access to buyer reasoning at the moment of peak clarity — but most programs fail because interviews aren't conducted well, findings aren't synthesized rigorously, or results aren't communicated in a way that drives action.

## When to Use This Skill
- Win rates are declining and you don't know why
- Sales team is making the same mistakes repeatedly
- Product roadmap debates need real buyer data
- A specific competitor keeps appearing in losses
- You're entering a new segment and need to understand buyer criteria
- Building or formalizing a win/loss program
- Preparing for a board review or pricing analysis

## Why Win/Loss Analysis Works — and Why Most Programs Fail

**Why it works:**
Buyers who've just made a decision are in a unique state. They've synthesized all their research, done the comparison, and made a judgment. That judgment — the reasons they articulate right after deciding — is the closest thing to ground truth about what drove the decision.

**Why programs fail:**
1. **Sales reps conduct the interviews** — They have relationships to protect, deals to recover, and biases toward self-serving answers. Buyers lie politely to reps.
2. **Interviews happen too late** — Memory degrades fast. Interviews more than 90 days after the decision produce rationalizations, not reasons.
3. **Only losses are studied** — Wins reveal what you're doing right and what's actually working. Without win data, you can't distinguish signal from luck.
4. **Findings aren't actioned** — A 40-slide deck shared to a Confluence page nobody reads isn't analysis — it's data accumulation.
5. **Programs are reactive, not systematic** — Analyzing every deal when a competitor shows up vs. systematically sampling the deal population

## Program Design Decisions

Before the first interview, answer these questions:

### Who Conducts the Interviews?
**Best options (in order):**
1. **Third-party firm** — Most candid responses; buyers will say things to a neutral party they won't say to the vendor
2. **PMM** — Better than sales because not tied to the deal; reasonably neutral
3. **Customer research function** — UX researchers are trained interviewers; good if available
4. **Never**: The sales rep who worked the deal, or anyone who might recover the account

### Which Deals to Interview?
**Sampling strategy:**
- **Random sample, not volunteer sample** — Don't only interview happy winners and cooperative prospects
- **All tiers**: Include T1 deals (large) and T2 deals (mid-market); don't only study big deals
- **Equal wins and losses**: A 50/50 ratio is ideal; many programs over-sample losses
- **Time filter**: Only interview deals closed within the last 90 days
- **Minimum volume**: 3-5 interviews/month to see patterns; 1/month is too thin to generalize

### How Many to Interview per Quarter?
- **Starting out**: 6-10 interviews per quarter to establish baseline
- **Ongoing**: 4-8 per quarter to track trends
- **Focused study**: 8-12 interviews if investigating a specific competitor or segment

### How to Recruit Interviewees?
**For losses:**
- Sales ops pulls closed-lost deals by date
- PMM or neutral party reaches out directly (not the sales rep)
- Frame: "We're doing research to understand how buyers make decisions in our market. Not trying to re-open the deal — we'd genuinely like to learn from your experience."

**For wins:**
- CS or sales ops pulls recently closed-won deals
- Ask via CSM or account executive intro to warm the outreach
- Frame: "We'd love to understand what made your decision process go the way it did — what you were looking for and how you evaluated options."

**Incentive:** Consider a $50-100 gift card. It signals their time is valued and improves response rates.

## The Interview Guide

### Opening Script

"Thanks for taking the time. To give you a bit of context: I'm doing research to understand how [type of buyer] evaluates and makes decisions in [category]. I'm not here to re-open the deal or sell you anything. I want to understand your experience — what you were trying to accomplish, how you evaluated your options, and what drove your final decision.

There are no wrong answers. I'll be taking notes but everything you share stays in aggregate research — I won't share specific quotes with your name attached without your permission.

This should take about 30-45 minutes. Sound good?"

### Section 1: Context Setting

**Goal:** Understand what the buyer was trying to accomplish and why they started looking.

Questions:
- "Start at the beginning — what was going on in your organization that led you to start evaluating solutions in this space?"
- "What problem were you trying to solve? What did success look like?"
- "How long had this been a problem before you decided to do something about it?"
- "Who else was involved in the evaluation — who were the key stakeholders?"
- "What was the urgency? Was there an event or deadline that created pressure?"

**What you're learning:**
- The real business problem (may differ from how sales characterized it)
- The buying committee composition
- The urgency driver
- Whether the problem aligns with your ICP or reveals a segment mismatch

### Section 2: Evaluation Criteria

**Goal:** Understand what factors drove the decision — both what they said they cared about and what actually mattered.

Questions:
- "When you started evaluating, what were the most important requirements?"
- "Were there absolute must-haves vs. nice-to-haves? What would have immediately disqualified a vendor?"
- "Did your criteria change during the evaluation? What shifted?"
- "What surprised you about what ended up mattering most in the final decision?"
- "Looking back, what were the top 2-3 factors that drove your decision?"

**What you're learning:**
- The actual decision criteria (often different from stated criteria at beginning of evaluation)
- The criteria that differentiate winners from losers
- What criteria are table stakes vs. what creates competitive advantage
- Where your marketing may be emphasizing the wrong things

### Section 3: The Competitive Landscape

**Goal:** Understand who they evaluated and how you compared.

Questions:
- "Who else did you evaluate? Who was on your shortlist?"
- "How did you first hear about [your company]? And about [competitors]?"
- "How did [your company] compare to the other options you considered?"
- "Were there specific capabilities or approaches that stood out — positively or negatively — with any vendor?"
- "Was there any vendor you dismissed early? What disqualified them?"
- "What was the reputation of [your company] vs. [competitor] going in?"

**For wins:**
- "What was it about [your company] that put you over the top?"
- "Was there a specific moment in the evaluation where [your company] pulled ahead?"

**For losses:**
- "What was it about [chosen vendor] that made them the right fit?"
- "Was there something [your company] could have done differently that might have changed the outcome?"
- "What was the main reason [your company] wasn't selected?"

**What you're learning:**
- Real competitive set (who shows up most often)
- Where you win and where you lose the comparison
- What competitors are doing well that you're not
- Whether you're losing on product, price, process, or perception

### Section 4: The Sales Experience

**Goal:** Understand how the sales process influenced the decision.

Questions:
- "How was your experience with [company's] sales team?"
- "Were there moments in the process where you felt very confident about [company]? Or where you lost confidence?"
- "Did you feel like [company]'s team understood your situation well?"
- "Was the evaluation process itself easy or difficult? How did it compare to other vendors?"
- "Was there anything about the process — demos, trial, references, pricing discussions — that stood out?"

**What you're learning:**
- Whether you're losing deals the product could win (process/rep failures)
- What's building vs. eroding trust during the sales process
- Whether your demo is effective
- Pricing and contracting friction

### Section 5: Price and Value

**Goal:** Understand whether price was a factor and what drove the value perception.

Questions:
- "How did pricing factor into your decision?"
- "Did the pricing feel fair given what was being offered?"
- "Were there any surprises in the pricing discussions?"
- "How did [company]'s pricing compare to the alternatives you considered?"

**What you're learning:**
- Whether you're losing on price vs. on value perception
- Whether your pricing model creates friction
- Whether discounting is being demanded vs. volunteered

### Section 6: What They'd Tell a Peer

**Goal:** The most honest synthesis — what would they tell a colleague considering the same decision?

Questions:
- "If a colleague came to you and said they were evaluating solutions in this space, what advice would you give them?"
- "What should they look for? What should they watch out for?"
- "If they asked about [your company], what would you say?"
- "Is there anything you'd do differently if you were going through this evaluation again?"

**What you're learning:**
- The clearest summary of perception — unfiltered by trying to be polite
- Whether what you believe is a strength is actually how you're perceived
- Word-of-mouth narrative about your company

### Closing

"This has been incredibly helpful. Before we wrap up — is there anything you wanted to share that I didn't ask about? Or anything you want to make sure I understand?"

"One last question: would you mind if I reached back out if I have a follow-up question in the next few weeks?"

## Synthesizing Findings

### After Each Interview

Within 24 hours, write a single-page summary:
- What drove the decision (stated reason + your interpretation)
- Key competitive mentions
- Evaluation criteria that mattered
- Sales process observation
- Price/value perception
- Most memorable quote

### After a Batch of 6-10 Interviews

Look for patterns across:

**Win patterns** — What's consistently present in wins but absent in losses:
- Specific evaluation criteria where you lead
- Segments/industries where you win disproportionately
- Sales process moments that consistently build confidence

**Loss patterns** — What's consistently present in losses:
- Specific criteria where competitors win
- Segments/industries with below-average win rate
- Sales process moments that consistently create doubt

**Surprise findings** — What doesn't match your assumptions:
- A competitor you didn't know you were losing to
- A criterion that matters more than you thought
- A perception gap between what you believe about your product and how buyers see it

### The Findings Report Structure

A win/loss report should be **short enough to be read and specific enough to be acted upon.** The enemy of action is a 40-slide deck.

**Executive version (2 pages or 10 slides):**
1. What we learned (3 bullets)
2. Top reasons we win (3 bullets with examples)
3. Top reasons we lose (3 bullets with examples)
4. Competitive dynamics (which competitors are winning what, and why)
5. Biggest surprise finding
6. 3 recommendations with owner and timeline

**Detailed version for PMM/Product/Sales:**
- Same structure, but with supporting evidence (paraphrased quotes, frequency data)
- Competitive section with detail per named competitor
- Sales process findings
- Product gaps mentioned in interviews
- Segment-specific patterns

### Scoring and Tracking

For each interview, score on key factors:
- Primary reason for win/loss (product, price, process, perception, relationship)
- Competitive intensity (1-3: how hard did you fight for it?)
- Evaluation criteria mentioned (tag for trend analysis)
- Competitor mentioned (track frequency)

Over time, this enables:
- Win rate by segment
- Win rate by competitor
- Trend analysis: are win rates improving or declining?
- Attribution: is this a product problem, a pricing problem, or a sales problem?

## Communicating to Stakeholders

**Different stakeholders need different things:**

**Sales team:** Actionable intelligence they can use now
- "When [Competitor] comes up, here's what buyers say they do better, and here's how you respond"
- "The biggest reason we're losing deals in this segment is [X] — here's what to do about it"
- Share via Slack, sales meetings, or enablement sessions — not Confluence articles

**Product team:** Evidence for roadmap prioritization
- "Buyers consistently mention [capability gap] when explaining why they chose competitors"
- "We're winning despite [limitation], which means it's not blocking deals — don't prioritize based on rep requests for it"
- Frame as buyer evidence, not as PMM opinion

**Leadership:** Business impact and strategic implications
- Win rate trends and what's driving them
- Competitive threats and how they're evolving
- Strategic recommendations with expected impact

**The rule:** If findings don't lead to changed behavior in sales, product, or marketing, the program isn't working. Go back and figure out why the findings aren't landing.

## Output Template

```
# Win/Loss Analysis Report
Quarter: [Q] | Deals analyzed: [N] | Wins: [N] | Losses: [N]
Analyst: [Name] | Date: [Date]

---

## Three Things We Learned This Quarter
1. [Specific finding with supporting evidence]
2. [Specific finding with supporting evidence]
3. [Specific finding with supporting evidence]

---

## Why We Win

### Top Win Factor 1: [Factor name]
What we hear: "[Paraphrased customer language]"
Frequency: Mentioned in [X] of [Y] win interviews
What this means: [Implication for messaging/sales/product]

### Top Win Factor 2: [Factor name]
...

### Top Win Factor 3: [Factor name]
...

---

## Why We Lose

### Top Loss Factor 1: [Factor name]
What we hear: "[Paraphrased customer language]"
Frequency: Mentioned in [X] of [Y] loss interviews
Root cause: [Product gap? Sales gap? Perception gap? Pricing?]
Recommendation: [Specific action]

### Top Loss Factor 2: [Factor name]
...

### Top Loss Factor 3: [Factor name]
...

---

## Competitive Intelligence

### [Competitor A]
Win rate against them: [%]
What they win on: [Top reasons]
What we win on: [Top reasons]
Pattern/trend: [Any change vs. prior quarter]

### [Competitor B]
...

---

## Sales Process Observations
What's building confidence: [Pattern]
What's creating doubt: [Pattern]
Recommendation: [Specific training or process change]

---

## Recommendations

| Recommendation | Owner | Timeline | Expected Impact |
|----------------|-------|----------|----------------|
| [Action] | [PMM/Sales/Product] | [Q] | [Outcome] |
| [Action] | [PMM/Sales/Product] | [Q] | [Outcome] |
| [Action] | [PMM/Sales/Product] | [Q] | [Outcome] |

---

## Next Quarter Focus
[What we're investigating next and why]
```

## Best Practices

### Do's
- **Conduct interviews within 90 days of the decision** — Memory is accurate; rationalizations haven't set in
- **Interview both wins and losses** — A win-only program misses what's actually working vs. what's a lucky outcome
- **Have a neutral party conduct interviews** — The closer to the deal, the less candid the response
- **Share findings quickly** — A 3-month analysis cycle is too slow to drive behavior change
- **Quantify frequency** — "Several buyers mentioned" is weaker than "7 of 12 buyers mentioned"
- **Make recommendations specific** — Not "improve demos" but "revise the discovery question sequence for [segment]"

### Don'ts
- **Don't only interview recent losses** — You'll develop a distorted view of what drives decisions
- **Don't let the program become a blame mechanism** — Win/loss is about learning, not judging reps
- **Don't share raw interview transcripts with sales** — Buyers were candid because they trusted neutrality; protect that
- **Don't confuse stated reason with actual reason** — Buyers sometimes cite price because it's easier than saying "your team didn't understand my problem"
- **Don't run win/loss in isolation from pipeline data** — Connect interview findings to deal data for statistical significance

## Common Pitfalls

**Pitfall 1: The "we lost because of price" fallacy**
Price is the most commonly cited reason for loss, but it's often a proxy for "we didn't see enough value to justify the cost"
→ Solution: When buyers cite price, probe: "If the price had been the same, would you have chosen [company]?" If no, the real issue is elsewhere.

**Pitfall 2: Analyzing exceptions, not patterns**
One vivid loss in an enterprise deal gets analyzed in depth while 20 mid-market losses accumulate unexamined
→ Solution: Sample systematically, not opportunistically. Volume reveals patterns.

**Pitfall 3: Findings that can't be acted on**
"Buyers want a better product" is a finding. It's not actionable.
→ Solution: For every finding, push to: what specifically, in what segment, driving what behavior, fixable how?

**Pitfall 4: Reporting backwards from the conclusion**
PMM already believes [Competitor] is winning on [feature]. Interviews confirm it. Nothing else gets surfaced.
→ Solution: Code interviews systematically against a standard framework; don't filter through pre-existing hypotheses.

## Interactive Approach

When guiding someone through this framework, Claude should:

1. **Scope the program** — What specific question should this round of research answer?
2. **Design the sample** — Which deals, how many, wins vs. losses ratio?
3. **Adapt the interview guide** — Which sections matter most for this goal?
4. **Coach on interview technique** — How to probe without leading, how to handle evasive answers
5. **Guide synthesis** — Pattern recognition across interviews, distinguishing signal from noise
6. **Build the report** — Specific findings, competitive implications, and actionable recommendations
7. **Plan stakeholder communication** — Who needs what, in what format, delivered how?

## References
- Clozd. ["A Framework for Creating the Right Win Loss Analysis Questions."](https://www.clozd.com/blog/a-framework-for-creating-the-right-win-loss-analysis-questions) 2024.
- Pragmatic Institute. ["Eight Win-Loss Analysis Best Practices."](https://www.pragmaticinstitute.com/resources/articles/product/eight-win-loss-analysis-best-practices/) 2024.
- Product Marketing Alliance. ["How to Conduct Effective Win-Loss Interviews."](https://www.productmarketingalliance.com/how-to-conduct-win-loss-interviews/) 2024.
- Klue. ["The Ultimate 7-Step Guide to Win-Loss Analysis."](https://klue.com/blog/win-loss-analysis-guide) 2024.

## Skill Invocation Strategy

When invoked, guide the user through:
1. Defining the program goal and research question
2. Designing the sampling and recruitment approach
3. Adapting the interview guide for their context
4. Synthesizing findings into patterns
5. Building a report structured for action
6. Planning stakeholder communication

The goal is a program that changes what the business does — improves win rates, informs the roadmap, and sharpens competitive positioning. Not a research archive.
